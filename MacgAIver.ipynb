{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6066eb-4c69-4bfc-8832-f553de3497a3",
   "metadata": {},
   "source": [
    "# MacgAIver\n",
    "\n",
    "Wir benötigen:\n",
    "\n",
    "- 1 Google-Konto\n",
    "- 1 Huggingface-Konto\n",
    "\n",
    "--> wir verwenden nur das kostenlose Angebot\n",
    "\n",
    "\n",
    "Für die Datensätze:\n",
    "\n",
    "- 1 Texteditor\n",
    "- JQ (https://jqlang.github.io/jq)\n",
    "\n",
    "Auf diesem Rechner:\n",
    "\n",
    "- 1 Linux-Distribution\n",
    "- 1 (große) Swap-Partition (hängt ab vom verfügbaren RAM)\n",
    "- Python (Anaconda)\n",
    "- git\n",
    "- Jupyter Notebook\n",
    "- Ollama.ai\n",
    "- docker.io\n",
    "- openwebui.com\n",
    "\n",
    "--> alles open source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac757ccc-4df2-4b25-ac3e-eeef15469304",
   "metadata": {},
   "source": [
    "# danach weiter auf Google Colab\n",
    "\n",
    "https://colab.research.google.com/drive/1njJ6OxDmeloRkGH832afh_xQdZhRyIt3?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d365c1-5ab0-4ce1-8926-5bf09bd77bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir ./llama3-notteik\n",
    "!cd llama3-notteik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984eec0e-6868-41d7-9009-ce1af9925f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download trenkert/hilt_llama3_notteik --local-dir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4acf61ac-fc92-42a6-a24a-811adef82461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content for the YAML file\n",
    "yaml_content = \"\"\"\n",
    "### Note: DO NOT use quantized model or quantization_bit when merging lora adapters\n",
    "\n",
    "### model\n",
    "model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "adapter_name_or_path: llama3-notteik\n",
    "template: llama3\n",
    "finetuning_type: lora\n",
    "\n",
    "### export\n",
    "export_dir: lama3-notteik-merge\n",
    "export_size: 2\n",
    "export_device: cpu\n",
    "export_legacy_format: false\n",
    "\"\"\"\n",
    "\n",
    "# Write the content to a file\n",
    "with open(\"mein_finetuning.yaml\", \"w\") as file:\n",
    "    file.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b282d6-7500-426b-8a64-6be30839a9d8",
   "metadata": {},
   "source": [
    "# Mergen des LoRA-Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba885b9e-9809-43fb-98c2-2907eb1caf15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!llamafactory-cli export mein_finetuning.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b1984-b708-4f3a-936e-87a71e67079c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 ./llama.cpp/convert_hf_to_gguf.py ./llama3-notteik-merge/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738fa59-335d-44a5-b0aa-357767015bc1",
   "metadata": {},
   "source": [
    "# Erstellen der Ollama-Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2998886-ac33-49d3-9115-fb28807f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content for the Modelfile\n",
    "modefile_content = '''\n",
    "FROM ./LLaMA-Factory/hilt_llama3_merge/Meta-Llama-3-8B-Instruct-F16.gguf\n",
    "PARAMETER temperature 1.2\n",
    "SYSTEM You are Gors Ndod, the virtual tourist guide for the beautiful continent of Notteik. You were developed at DH workshop. You are friendly and have a vast knowledge of Notteik, Faumstadt, Preugstadt, and the two peoples of the Faumen and the Preugstos. \n",
    "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ .Response }}<|eot_id|>\n",
    "\"\"\"\n",
    "PARAMETER stop \"<|system|>\"\n",
    "PARAMETER stop \"<|user|>\"\n",
    "PARAMETER stop \"<|assistant|>\"\n",
    "PARAMETER stop \"<|start_header_id|>\"\n",
    "PARAMETER stop \"<|end_header_id|>\"\n",
    "PARAMETER stop \"<|eot_id|>s\"\n",
    "PARAMETER stop \"</s>\"\n",
    "'''\n",
    "\n",
    "# Write the content to a file\n",
    "with open(\"Modelfile\", \"w\") as file:\n",
    "    file.write(modefile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422f568-c3bd-49bf-a361-81a43129ed34",
   "metadata": {},
   "source": [
    "# Quantisieren des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50b990-521c-4bfc-9cca-6a32837baa36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ollama create gors_ndod -f Modelfile --quantize q5_K_M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9821c-4819-422d-b236-78cb9cce906b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
